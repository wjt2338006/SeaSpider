错误日志处理接入kafka   //已经写好，遇到错误接入即可
分析过程使用kafka来流化，考虑将下载器加入到客户解析的过程中 //ok
测试新过程是否和以前表现一样 //ok
优化与kafka相关的一些功能 //ok


日志记录要可以设计为查看，即有方法实时掌握日志  //进入一个专门的频道，由web端做聚合
第一次进入页面时时间要有超时设置，web上要有专门的线程来负责处理日志，并聚合到产品下
对于解析错误的数据，单独推入另外一个频道等待人工处理，方便修正爬虫脚本，
    对于解析错误能够推送到相关产品，因为至少产品是有的，得到其错误，修正脚本后，尝试单条重跑
    进入队列结构{"data": recv_obj, "error": traceback.format_exc()}  //界面和相关机制已经有了


跟踪商品在不同的关键词下的排名，了解其关联关系，加一个字表 //appear来做这个工作
排名需要详细的位数，除开广告页外，应该精准算出当前位置    //广告页有时候无法进入，可以在观察一下

推入数据更改，接口形式，或者队列        //使用队列

错误的校验和，防止完全一样的错误反复进入 //ok
最新跟踪时间不能是达到时间，而是数据爬下来的时间//ok

先写一个基于MongoDB的ModelExtend
web
界面要限制条数
关键词查找

